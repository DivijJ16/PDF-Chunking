{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divij\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pytesseract\n",
    "import google.generativeai as genai\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image, display\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "genai.configure(api_key=\"AIzaSyBHsmeRUUxi0EqiUy2D9Pm1kavdPanCk1Q\")\n",
    "vision_model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF \n",
    "from math import ceil \n",
    "from IPython.display import Image, display\n",
    "\n",
    "class PdfChunker: \n",
    "    def __init__(self, pdf_path, chunk_size=3): \n",
    "        \"\"\" \n",
    "        Initialize the PDF chunker \n",
    "         \n",
    "        Args: \n",
    "            pdf_path (str): Path to the PDF file \n",
    "            chunk_size (int, optional): Number of pages per chunk. Defaults to 3. \n",
    "        \"\"\" \n",
    "        self.doc = fitz.open(pdf_path) \n",
    "        self.total_pages = len(self.doc) \n",
    "        self.chunk_size = chunk_size \n",
    "        self.chunks = ceil(self.total_pages / chunk_size)  # Calculate total number of chunks \n",
    "         \n",
    "    def get_chunk(self, chunk_number): \n",
    "        \"\"\" \n",
    "        Get a specific chunk of pages as images \n",
    "         \n",
    "        Args: \n",
    "            chunk_number (int): The chunk number (0-based index) \n",
    "             \n",
    "        Returns: \n",
    "            list: List of PNG image data for the requested chunk \n",
    "        \"\"\" \n",
    "        if chunk_number >= self.chunks: \n",
    "            raise ValueError(f\"Chunk number should be between 0 and {self.chunks-1}\") \n",
    "             \n",
    "        start_page = chunk_number * self.chunk_size \n",
    "        end_page = min((chunk_number + 1) * self.chunk_size, self.total_pages) \n",
    "         \n",
    "        chunk_images = [] \n",
    "        for page_num in range(start_page, end_page): \n",
    "            page = self.doc[page_num] \n",
    "            pix = page.get_pixmap() \n",
    "            img_data = pix.tobytes(\"png\") \n",
    "            chunk_images.append(img_data) \n",
    "             \n",
    "        return chunk_images \n",
    "     \n",
    "def pdf_to_images(pdf_path, pages_per_chunk=3): \n",
    "    \"\"\"Convert PDF into images and store them in memory as chunks.\"\"\" \n",
    "    chunker_obj = PdfChunker(pdf_path, pages_per_chunk)  # Pass chunk_size  \n",
    "    image_chunks = [] \n",
    "    total_chunks = chunker_obj.chunks \n",
    "    for i in range(total_chunks): \n",
    "        image_chunks.append(chunker_obj.get_chunk(i)) \n",
    "    return image_chunks \n",
    "\n",
    "# # Example usage with display\n",
    "# def display_pdf_images(pdf_path, pages_per_chunk=3):\n",
    "#     \"\"\"Display PDF images chunk by chunk\"\"\"\n",
    "#     image_chunks = pdf_to_images(pdf_path, pages_per_chunk)\n",
    "    \n",
    "#     # Display each chunk of images\n",
    "#     for chunk_index, chunk in enumerate(image_chunks):\n",
    "#         print(f\"Chunk {chunk_index + 1}:\")\n",
    "#         for page_index, img_data in enumerate(chunk):\n",
    "#             display(Image(data=img_data))\n",
    "\n",
    "# # Uncomment and replace with your PDF path\n",
    "# display_pdf_images(\"test_pdf.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 7/7 [05:09<00:00, 44.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insights saved to insights.json\n",
      "Chunk summaries saved to chunk_summaries.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_images(image_paths, global_insights, chapter_insights):\n",
    "    \"\"\"Analyze images while referring to global and chapter-level insights.\"\"\"\n",
    "    from PIL import Image\n",
    "    import io\n",
    "\n",
    "    # Convert byte image data to PIL Image objects\n",
    "    image_data = [Image.open(io.BytesIO(img_bytes)) for img_bytes in image_paths]\n",
    "\n",
    "    # Contextual prompt for Gemini with explicit chapter detection\n",
    "    chapter_detection_prompt = f\"\"\"\n",
    "    Carefully examine these images and perform the following tasks:\n",
    "\n",
    "    1. Chapter Detection:\n",
    "    - Identify any clear chapter headings or titles in the images\n",
    "    - Look for page headers, section markers, or explicit chapter indicators\n",
    "    - Note the exact chapter name or number as it appears in the image\n",
    "    - If multiple chapters are present, list them in order of appearance\n",
    "    - If no chapter is present in these images, don't output any chapter\n",
    "\n",
    "    2. Comprehensive Image Analysis:\n",
    "    - Provide a thorough analysis of the key insights present in these images\n",
    "    - The analysis must be deep and cover all available information\n",
    "    - Separate different themes into distinct paragraphs\n",
    "    - Avoid unnecessary introductory phrases\n",
    "    - DO NOT use unnecessary phrases like: ## Analysis of Text Chunk: Sections 2.9 and 2.10\\n\\nThis text chunk,**Comprehensive Image Analysis:**,..\n",
    "\n",
    "\n",
    "    Previous Context:\n",
    "    - Global insights so far: {global_insights[:]}\n",
    "    - Current chapter insights: {chapter_insights[-5:]}\n",
    "\n",
    "    Output Format:\n",
    "    First, list the detected chapter (if any)\n",
    "    Only list one detected chapter per one set.\n",
    "    Then, provide a detailed analysis of the image contents\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate response with chapter detection and image analysis\n",
    "    response = vision_model.generate_content([chapter_detection_prompt] + image_data)\n",
    "    return response.text if response else \"\"\n",
    "\n",
    "def detect_chapters(text):\n",
    "    \"\"\"Detect chapters based on the chapter detection part of the image analysis.\"\"\"\n",
    "    # Extract chapter information from the first line of the image analysis\n",
    "    chapter_lines = text.split('\\n')[0].strip()\n",
    "    \n",
    "    # Look for chapter patterns\n",
    "    chapter_pattern = r\"\\b(?:Chapter|CHAPTER|CHAP)\\s+\\d+.*\"\n",
    "    detected_chapters = re.findall(chapter_pattern, chapter_lines)\n",
    "    \n",
    "    return detected_chapters if detected_chapters else [chapter_lines]\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=50):\n",
    "    \"\"\"Splits a large text into chunks of approximately 'chunk_size' words.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def summarize_text(text_chunk, previous_chapter_insights, current_chapter, global_context):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of a text chunk considering contextual insights.\n",
    "    \n",
    "    Args:\n",
    "        text_chunk (str): The current 50-word chunk to be analyzed\n",
    "        previous_chapter_insights (list): Insights from previous chunks in the current chapter\n",
    "        current_chapter (str): Name of the current chapter\n",
    "        global_context (list): Global insights collected so far\n",
    "    \n",
    "    Returns:\n",
    "        str: Comprehensive analysis of the text chunk\n",
    "    \"\"\"\n",
    "    # Prepare previous chapter insights as context\n",
    "    previous_insights_context = \" \".join(previous_chapter_insights) if previous_chapter_insights else \"No previous insights in this chapter.\"\n",
    "\n",
    "    # Construct a detailed prompt that incorporates multiple layers of context\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following text chunk with extreme thoroughness. Your analysis should:\n",
    "    1. Extract key insights specific to this text chunk\n",
    "    2. Contextualize the chunk within the chapter's previous insights\n",
    "    3. Highlight connections to previous insights\n",
    "    4. Do NOT use unnecessary phrases like: ## Analysis of Text Chunk: Sections 2.9 and 2.10\\n\\nThis text chunk,**Comprehensive Image Analysis:**,..\n",
    "\n",
    "    Current Chapter: {current_chapter}\n",
    "\n",
    "    Previous Chapter Insights:\n",
    "    {previous_insights_context}\n",
    "\n",
    "    Global Context Hints:\n",
    "    {' '.join(global_context[-3:])}\n",
    "\n",
    "    Text Chunk to Analyze:\n",
    "    {text_chunk}\n",
    "\n",
    "    Provide a comprehensive, structured analysis that:\n",
    "    - Identifies core themes and concepts\n",
    "    - Explains how this chunk relates to previous insights in the chapter\n",
    "    - Highlights any new or unique information\n",
    "    - Connects insights across different parts of the text\n",
    "    \"\"\"\n",
    "\n",
    "    # Use vision model to generate a comprehensive analysis\n",
    "    response = vision_model.generate_content(prompt)\n",
    "    return response.text if response else \"\"\n",
    "\n",
    "def process_pdf(pdf_path, output_json=\"insights.json\", summary_json=\"chunk_summaries.json\"):\n",
    "    \"\"\"Process PDF into subparts, analyze, detect chapters, store insights, and generate summaries.\"\"\"\n",
    "    image_chunks = pdf_to_images(pdf_path)  # No output folder, just a list of image lists\n",
    "    insights = {\"global\": [], \"chapters\": {}}\n",
    "    chunk_summaries = {}\n",
    "    current_chapter = \"Introduction\"  # Default if no chapter is found\n",
    "    chapter_chunk_insights = []  # Tracks insights for the current chapter\n",
    "\n",
    "    for i, image_group in enumerate(tqdm(image_chunks, desc=\"Processing chunks\")):\n",
    "        # Analyze image while considering global and chapter-level insights\n",
    "        sub_local_insight = analyze_images(\n",
    "            image_group,\n",
    "            insights[\"global\"],\n",
    "            insights[\"chapters\"].get(current_chapter, [])\n",
    "        )\n",
    "\n",
    "        # Detect chapters directly from the image analysis output\n",
    "        detected_chapters = detect_chapters(sub_local_insight)\n",
    "        if detected_chapters:\n",
    "            # If a new chapter is detected, reset chapter-specific insights\n",
    "            current_chapter = detected_chapters[-1]  # Use last detected chapter name\n",
    "            chapter_chunk_insights = []  # Reset insights for the new chapter\n",
    "\n",
    "        # Chunk text into 50-word segments\n",
    "        text_chunks = split_text_into_chunks(sub_local_insight, chunk_size=50)\n",
    "\n",
    "        # Store insights by chapter\n",
    "        if current_chapter not in insights[\"chapters\"]:\n",
    "            insights[\"chapters\"][current_chapter] = []\n",
    "\n",
    "        insights[\"chapters\"][current_chapter].extend(text_chunks)\n",
    "\n",
    "        # Generate comprehensive summary for each chunk\n",
    "        for chunk_number, chunk in enumerate(text_chunks, 1):\n",
    "            # Pass previous chapter chunk insights before the current chunk\n",
    "            previous_insights = chapter_chunk_insights.copy()\n",
    "            \n",
    "            chunk_summary = summarize_text(\n",
    "                chunk, \n",
    "                previous_insights, \n",
    "                current_chapter, \n",
    "                insights[\"global\"]\n",
    "            )\n",
    "            chunk_summaries[f\"{current_chapter}_Chunk_{chunk_number}\"] = chunk_summary\n",
    "            \n",
    "            # Add current chunk's insight to chapter-specific insights\n",
    "            chapter_chunk_insights.append(chunk_summary)\n",
    "\n",
    "        # Update global insights\n",
    "        insights[\"global\"].append(sub_local_insight)\n",
    "\n",
    "    # Save insights JSON\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(insights, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Save chunk summaries JSON\n",
    "    with open(summary_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_summaries, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Insights saved to {output_json}\")\n",
    "    print(f\"Chunk summaries saved to {summary_json}\")\n",
    "\n",
    "# Example Usage:\n",
    "pdf_file = \"david_ch12.pdf\"\n",
    "process_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
